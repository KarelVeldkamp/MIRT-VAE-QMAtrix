{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a8be01-ae73-4a32-8c37-93182ca7789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/karel/Documents/GitHub/VAE-MIRT-Missing/MIRTVAE/data.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_train = torch.tensor(X, dtype=torch.float32)\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | encoder   | Encoder       | 2.1 K \n",
      "1 | sampler   | SamplingLayer | 0     \n",
      "2 | transform | CholeskyLayer | 9     \n",
      "3 | decoder   | Decoder       | 240   \n",
      "--------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=10` in the `DataLoader` to improve performance.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1549: 100%|████████████████████████████████| 1/1 [00:00<00:00, 151.27it/s, v_num=11]\n",
      "0.5482209215483743\n",
      "transforming theta\n",
      "[[ 1.         -0.74818376 -0.50329812]\n",
      " [-0.74818376  1.          0.63687801]\n",
      " [-0.50329812  0.63687801  1.        ]]\n",
      "0.7868637838948477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karel/Documents/GitHub/VAE-MIRT-Missing/MIRTVAE/data.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_train = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from model import *\n",
    "from data import *\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "##############\n",
    "# Data generation\n",
    "###############\n",
    "N = 1000  # number of subjects\n",
    "nit = 60  # number of items\n",
    "ndim = 3  # number of dimensions\n",
    "nit_dim = int(nit / ndim)  # number of items per dimension\n",
    "max_nepoch = 50000  # max number of iterations\n",
    "\n",
    "X = np.full((N, nit), 999)  # empty matrix for item scores\n",
    "prob = np.full((N, nit), 0.99)  # empty matrix for probability correct\n",
    "a = np.full((nit, ndim), 0.0)  # empty matrix for discrimination parameters\n",
    "\n",
    "covMat = np.full((ndim, ndim), .3)  # covariance matrix of dimensions\n",
    "np.fill_diagonal(covMat, 1)\n",
    "theta = np.random.multivariate_normal([0] * ndim, covMat, N)  # draw values for the dimensions\n",
    "\n",
    "structure_matrix = np.full((nit, ndim), 0.0)\n",
    "\n",
    "for i in range(0, ndim):\n",
    "    ax = np.random.uniform(.5, 1.5, nit_dim)  # draw discrimination parameters from uniform distribution\n",
    "    a[range(nit_dim * i, nit_dim * (i + 1)), i] = ax\n",
    "Q = np.zeros((60,3))  # simple structure configuration\n",
    "Q[0:20, 0] = 1\n",
    "Q[20:40, 1] = 1\n",
    "Q[40:60,2] = 1\n",
    "\n",
    "b = np.tile(np.linspace(-3, 3, nit_dim, endpoint=True), ndim)  # decoder intercepts\n",
    "\n",
    "for i in range(0, nit):\n",
    "    for p in range(0, N):\n",
    "        prob[p, i] = 1 / (1 + math.exp(-(sum(a[i, :] * theta[p, :]) + b[i])))  # probability correct\n",
    "        X[p, i] = np.random.binomial(1, prob[p, i])  # draw item scores on basis of prob correct\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs=50000,\n",
    "                  min_epochs=100,\n",
    "                  enable_checkpointing=False,\n",
    "                  logger=None,\n",
    "                  accelerator='cpu',\n",
    "                  callbacks=[EarlyStopping(monitor='train_loss', min_delta=1e-7, patience=50, mode='min')])\n",
    "\n",
    "dataset = SimDataset(torch.Tensor(X))\n",
    "train_loader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "vae = VAE(nitems=X.shape[1],\n",
    "           dataloader=train_loader,\n",
    "           latent_dims=3,\n",
    "           hidden_layer_size=(nit+ndim)//2,\n",
    "           qm=Q,\n",
    "           learning_rate=0.001,\n",
    "           batch_size=N,\n",
    "           n_samples=1,\n",
    "           cov_theta=True)\n",
    "\n",
    "trainer.fit(vae)\n",
    "\n",
    "a_est = vae.decoder.weights.t().detach().numpy()\n",
    "d_est = vae.decoder.bias.t().detach().numpy()\n",
    "\n",
    "print(((a_est-a)**2).mean())\n",
    "\n",
    "dataset = SimDataset(torch.Tensor(X))\n",
    "train_loader = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
    "data, mask = next(iter(train_loader))\n",
    "theta_est, log_sigma_est = vae.encoder(data)\n",
    "\n",
    "\n",
    "\n",
    "print('transforming theta')\n",
    "L = torch.tril(vae.transform.weight, -1) + torch.eye(vae.transform.weight.shape[0])\n",
    "theta_est = torch.matmul(theta_est, L)\n",
    "\n",
    "theta_est = theta_est.detach().numpy()\n",
    "\n",
    "a_est, theta_est = inv_factors(a_est=a_est, theta_est=theta_est, a_true=a)\n",
    "\n",
    "\n",
    "cor = np.corrcoef(theta_est.T)\n",
    "print(cor)\n",
    "\n",
    "\n",
    "est = np.array([cor[1,0], cor[2,0], cor[1,2]])\n",
    "true = np.array([covMat[1,0], covMat[2,0], covMat[1,2]])\n",
    "rmse_cor = np.sqrt(np.mean((est-true)**2))\n",
    "rmse_a = np.sqrt(np.mean((a_est-a)**2))\n",
    "print(rmse_cor)\n",
    "\n",
    "\n",
    "# for dim in range(3):\n",
    "#     plt.figure()\n",
    "#\n",
    "#     ai_est = a_est[:, dim]\n",
    "#     ai_true = a[:, dim]\n",
    "#\n",
    "#     mse = MSE(ai_est, ai_true)\n",
    "#     plt.scatter(y=ai_est, x=ai_true)\n",
    "#     plt.plot(ai_true, ai_true)\n",
    "#     # for i, x in enumerate(ai_true):\n",
    "#     #    plt.text(ai_true[i], ai_est[i], i)\n",
    "#     plt.title(f'Parameter estimation plot: a{dim + 1}, MSE={round(mse, 4)}')\n",
    "#     plt.xlabel('True values')\n",
    "#     plt.ylabel('Estimates')\n",
    "#     plt.savefig(f'./figures/param_est_plot_a{dim + 1}.png')\n",
    "#\n",
    "#     # parameter estimation plot for theta\n",
    "#     plt.figure()\n",
    "#     thetai_est = theta_est[:, dim].squeeze()\n",
    "#     thetai_true = theta[:, dim].squeeze()\n",
    "#     mse = MSE(thetai_est, thetai_true)\n",
    "#     plt.scatter(y=thetai_est, x=thetai_true)\n",
    "#     plt.plot(thetai_true, thetai_true)\n",
    "#     plt.title(f'Parameter estimation plot: theta{dim + 1}, MSE={round(mse, 4)}')\n",
    "#     plt.xlabel('True values')\n",
    "#     plt.ylabel('Estimates')\n",
    "#     plt.savefig(f'./figures/param_est_plot_theta{dim + 1}.png')\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a23bbb-316a-4c64-a629-dbc23140dd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.zeros((3,3))\n",
    "A[np.triu_indices(A.shape[0], k=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
