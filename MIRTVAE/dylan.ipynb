{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120749ed-3b62-446f-bef4-fe3072082308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "iteration = sys.argv[1]\n",
    "\n",
    "\n",
    "exit()\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import math as math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "from keras.constraints import UnitNorm\n",
    "from keras.layers import Lambda\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "ndim=3\n",
    "#############\n",
    "# functions #\n",
    "#############\n",
    "#specify function to sample from latent dim\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# cov2cor\n",
    "\n",
    "def cov2cor(x):\n",
    "    sds=np.diag(1/np.sqrt(np.diag(x)))\n",
    "    out=np.matmul(np.matmul(sds,x),sds)\n",
    "    return out\n",
    "\n",
    "#function to ensure simple structure\n",
    "def a_reg2(a):\n",
    "    b = a * structure_matrix\n",
    "    return b\n",
    "\n",
    "def inv_factors(a_est, a_true, theta_est=None):\n",
    "    \"\"\"\n",
    "    Helper function that inverts factors when discrimination values are mostly negative this improves the\n",
    "    interpretability of the solution\n",
    "        theta: NxP matrix of theta estimates\n",
    "        a: IxP matrix of a estimates\n",
    "\n",
    "        returns: tuple of inverted theta and a paramters\n",
    "    \"\"\"\n",
    "    for dim in range(a_est.shape[1]):\n",
    "        if pearsonr(a_est[:,dim], a_true[:,dim])[0] < 0:\n",
    "            a_est[:, dim] *= -1\n",
    "            theta_est[:, dim] *=-1\n",
    "\n",
    "    return a_est, theta_est\n",
    "\n",
    "#function to introduce covariances\n",
    "force_chol=K.cast(np.full((ndim,ndim),1),\"float32\")\n",
    "force_chol1=K.cast(np.tril(force_chol,k=-1),\"float32\")\n",
    "force_chol2=K.cast(np.diag([1]*ndim),\"float32\")\n",
    "\n",
    "def c_reg(c):\n",
    "    d= c*force_chol1\n",
    "    e=d+force_chol2\n",
    "    return e;   \n",
    "\n",
    "##############\n",
    "# Data generation\n",
    "###############\n",
    "N=1000                  # number of subjects\n",
    "nit=60                  # number of items \n",
    "ndim=3                  #number of dimensions\n",
    "nit_dim=int(nit/ndim)   #number of items per dimension\n",
    "max_nepoch=50000        #max number of iterations\n",
    "\n",
    "X=np.full((N,nit),999)      # empty matrix for item scores\n",
    "prob=np.full((N,nit),0.99)   # empty matrix for probability correct\n",
    "a=np.full((nit,ndim),0.0)      # empty matrix for discrimination parameters \n",
    "\n",
    "covMat=np.full((ndim,ndim),.3)  # covariance matrix of dimensions\n",
    "np.fill_diagonal(covMat,1)\n",
    "theta=np.random.multivariate_normal([0]*ndim,covMat,N)   # draw values for the dimensions\n",
    "\n",
    "structure_matrix=np.full((nit,ndim),0.0)\n",
    "\n",
    "for i in range(0,ndim):\n",
    "    ax=np.random.uniform(.5,1.5,nit_dim)            #draw discrimination parameters from uniform distribution\n",
    "    a[range(nit_dim*i, nit_dim*(i+1)),i] = ax\n",
    "    structure_matrix[range(nit_dim*i, nit_dim*(i+1)),i]=1\n",
    "structure_matrix = K.cast(K.transpose(structure_matrix),\"float32\")   # simple structure configuration\n",
    "\n",
    "    \n",
    "b=np.tile(np.linspace(-3,3,nit_dim,endpoint=True),ndim)   # decoder intercepts\n",
    "\n",
    "for i in range(0,nit):\n",
    "    for p in range(0,N):\n",
    "        prob[p,i]=1/(1+math.exp(-(sum(a[i,:]*theta[p,:])+b[i])))       # probability correct\n",
    "        X[p,i]=np.random.binomial(1,prob[p,i])                          # draw item scores on basis of prob correct\n",
    "\n",
    "##############\n",
    "# Set up VAE\n",
    "###############\n",
    "encoding_dim=np.ceil((nit+ndim)/2)\n",
    "latent_dim=ndim  #dimensions in VAE\n",
    "batch_size=N  # sample size \n",
    "\n",
    "\n",
    "x=Input(batch_shape=(batch_size, nit))    # input layer\n",
    "e1 = Dense(encoding_dim, activation='elu')(x)\n",
    "z_mean = Dense(latent_dim,activation='linear')(e1)                   # specify person mean latent dim\n",
    "z_log_sigma = Dense(latent_dim,activation='linear')(e1)              # specify person sd latent dim\n",
    "\n",
    "\n",
    "\n",
    "#sample from latent dim\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "#encoder: connect the input to the latent dim parameters\n",
    "encoder = Model(x, [z_mean, z_log_sigma, z])\n",
    "       \n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,))  # input for the decoder model: the latent dimensions\n",
    "#outputs1 = Dense(latent_dim, activation='linear', kernel_constraint=c_reg,use_bias=False)(latent_inputs)  # the \"cholesky layer\"\n",
    "outputs2 = Dense(nit, activation='sigmoid', kernel_constraint=a_reg2)(latent_inputs)  # connect input to output\n",
    " \n",
    " \n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs2)  # specify the decode model\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(x)[2])    # full output encoder -> decoder -> output\n",
    "vae = Model(x, outputs)\n",
    "\n",
    "\n",
    "# specify the loss function to estimate the model\n",
    "reconstruction_loss = binary_crossentropy(x,outputs)   \n",
    "\n",
    "reconstruction_loss *= nit\n",
    "kl_loss = 1 + 2*z_log_sigma - K.square(z_mean) - K.exp(2*z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    " \n",
    "# fit the model\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-8,patience=500,verbose=1,mode=\"min\")\n",
    "\n",
    "hist=vae.fit(X, \n",
    "        epochs=100,\n",
    "        batch_size=batch_size,verbose=0)\n",
    "\n",
    "hist=vae.fit(X, \n",
    "        epochs=max_nepoch,\n",
    "        batch_size=batch_size, verbose=0, callbacks=[callback])\n",
    "\n",
    "loss=hist.history['loss']\n",
    "\n",
    "pars=vae.get_weights()\n",
    "\n",
    "theta_est = encoder(X)\n",
    "\n",
    "##############\n",
    "# Check correlations\n",
    "###############\n",
    "\n",
    "# this *should* give the correct correlations (all .3):\n",
    "L=pars[-3]\n",
    "print(cov2cor(np.matmul(L,np.transpose(L))))\n",
    "\n",
    "#doesnt look like it. BUT:\n",
    "\n",
    "theta_est = np.matmul(theta_est[0],L)\n",
    "for i in range(3):\n",
    "    if np.corrcoef(theta_est[:,i], theta[:,i]) < 0:\n",
    "        theta_est *= -1\n",
    "\n",
    "np.corrcoef(np.transpose(theta_est))\n",
    "\n",
    "# using the biased lambdra matrix (L) and the estimates of mu\n",
    "# from the latent variables (g), the actual latent variables (f) seem to have the correct correlations \n",
    "\n",
    "# dit krijg ik als correlaties (die negatieve cors zijn niet erg, komt doordat schaal is omgeklapt, \n",
    "# dat kan ik ook zien aan de item ladingen, die zijn negatief voor factor 1 en 3:\n",
    " #   array([[ 1.        , -0.30513961,  0.39002258],\n",
    " #          [-0.30513961,  1.        , -0.32839606],\n",
    " #          [ 0.39002258, -0.32839606,  1.        ]])\n",
    " \n",
    " # en uit R (lavaan):\n",
    " # f1 ~~                                               \n",
    " # f2                0.290    0.038    7.670    0.000\n",
    " # f3                0.374    0.038    9.968    0.000\n",
    "#  f2 ~~                                               \n",
    "#    f3                0.317    0.039    8.196    0.000\n",
    "   \n",
    "#lijkt heel aardig overeen te komen, mÃ¡Ã¡r het zijn geen schattingen, de schattingen zelf (in L) zijn biased.\n",
    "#we moeten even nadenken hoe we dat willen oplossen (meerdere opties) \n",
    "     \n",
    "#heel soms worden de off diagnal elements in L groter dan 1. Dit zou geen probleem moeten zijn, maar dan is de boel flinkt\n",
    "# biased, bij mij kwam het 1x voor en toen waren de correlaties tussen de mu schattingen echt hoog. Hopelijk komt\n",
    "# dat niet vaak voor en anders moeten we misschien zorgen dat die off-diagonal elements tussen de -1 en 1 blijven.\n",
    "cor = cov2cor(np.matmul(L,np.transpose(L)))\n",
    "\n",
    "est = np.array([cor[1,0], cor[2,0], cor[1,2]])\n",
    "true = np.array([covMat[1,0], covMat[2,0], covMat[1,2]])\n",
    "\n",
    "mse = np.mean((est-true)**2)\n",
    "\n",
    "f = open(f\"test_{iteration}.txt\", \"w+\")\n",
    "f.write(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab30814-a375-4fd2-b68c-5eac779c7616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7136)\n",
      "tf.Tensor(0.71355796, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from keras.losses import binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "import torch\n",
    "\n",
    "true = np.array([1,1])\n",
    "pred = np.array([.4, .6])\n",
    "\n",
    "print(F.binary_cross_entropy(torch.Tensor(pred), torch.Tensor(true)))\n",
    "print(binary_crossentropy(K.constant(true), K.constant(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefc9913-8663-47ff-a5ae-5a8cdf166616",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpars\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pars' is not defined"
     ]
    }
   ],
   "source": [
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51886a-a48a-458e-9aa1-dc41724d32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_est[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36502ec-78bb-4cb7-9bb5-c2d4aede1eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2aa8f5b-790d-43f8-9394-5943e39f134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "iteration = sys.argv[1]\n",
    "\n",
    "\n",
    "exit()\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import math as math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "from keras.constraints import UnitNorm\n",
    "from keras.layers import Lambda\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "ndim=3\n",
    "#############\n",
    "# functions #\n",
    "#############\n",
    "#specify function to sample from latent dim\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# cov2cor\n",
    "\n",
    "def cov2cor(x):\n",
    "    sds=np.diag(1/np.sqrt(np.diag(x)))\n",
    "    out=np.matmul(np.matmul(sds,x),sds)\n",
    "    return out\n",
    "\n",
    "#function to ensure simple structure\n",
    "def a_reg2(a):\n",
    "    b = a * structure_matrix\n",
    "    return b\n",
    "\n",
    "def inv_factors(a_est, a_true, theta_est=None):\n",
    "    \"\"\"\n",
    "    Helper function that inverts factors when discrimination values are mostly negative this improves the\n",
    "    interpretability of the solution\n",
    "        theta: NxP matrix of theta estimates\n",
    "        a: IxP matrix of a estimates\n",
    "\n",
    "        returns: tuple of inverted theta and a paramters\n",
    "    \"\"\"\n",
    "    for dim in range(a_est.shape[1]):\n",
    "        if pearsonr(a_est[:,dim], a_true[:,dim])[0] < 0:\n",
    "            a_est[:, dim] *= -1\n",
    "            theta_est[:, dim] *=-1\n",
    "\n",
    "    return a_est, theta_est\n",
    "\n",
    "#function to introduce covariances\n",
    "force_chol=K.cast(np.full((ndim,ndim),1),\"float32\")\n",
    "force_chol1=K.cast(np.tril(force_chol,k=-1),\"float32\")\n",
    "force_chol2=K.cast(np.diag([1]*ndim),\"float32\")\n",
    "\n",
    "def c_reg(c):\n",
    "    d= c*force_chol1\n",
    "    e=d+force_chol2\n",
    "    return e;   \n",
    "\n",
    "##############\n",
    "# Data generation\n",
    "###############\n",
    "N=1000                  # number of subjects\n",
    "nit=60                  # number of items \n",
    "ndim=3                  #number of dimensions\n",
    "nit_dim=int(nit/ndim)   #number of items per dimension\n",
    "max_nepoch=50000        #max number of iterations\n",
    "\n",
    "X=np.full((N,nit),999)      # empty matrix for item scores\n",
    "prob=np.full((N,nit),0.99)   # empty matrix for probability correct\n",
    "a=np.full((nit,ndim),0.0)      # empty matrix for discrimination parameters \n",
    "\n",
    "covMat=np.full((ndim,ndim),.3)  # covariance matrix of dimensions\n",
    "np.fill_diagonal(covMat,1)\n",
    "theta=np.random.multivariate_normal([0]*ndim,covMat,N)   # draw values for the dimensions\n",
    "\n",
    "structure_matrix=np.full((nit,ndim),0.0)\n",
    "\n",
    "for i in range(0,ndim):\n",
    "    ax=np.random.uniform(.5,1.5,nit_dim)            #draw discrimination parameters from uniform distribution\n",
    "    a[range(nit_dim*i, nit_dim*(i+1)),i] = ax\n",
    "    structure_matrix[range(nit_dim*i, nit_dim*(i+1)),i]=1\n",
    "structure_matrix = K.cast(K.transpose(structure_matrix),\"float32\")   # simple structure configuration\n",
    "\n",
    "    \n",
    "b=np.tile(np.linspace(-3,3,nit_dim,endpoint=True),ndim)   # decoder intercepts\n",
    "\n",
    "for i in range(0,nit):\n",
    "    for p in range(0,N):\n",
    "        prob[p,i]=1/(1+math.exp(-(sum(a[i,:]*theta[p,:])+b[i])))       # probability correct\n",
    "        X[p,i]=np.random.binomial(1,prob[p,i])                          # draw item scores on basis of prob correct\n",
    "\n",
    "##############\n",
    "# Set up VAE\n",
    "###############\n",
    "encoding_dim=np.ceil((nit+ndim)/2)\n",
    "latent_dim=ndim  #dimensions in VAE\n",
    "batch_size=N  # sample size \n",
    "\n",
    "\n",
    "x=Input(batch_shape=(batch_size, nit))    # input layer\n",
    "e1 = Dense(encoding_dim, activation='elu')(x)\n",
    "z_mean = Dense(latent_dim,activation='linear')(e1)                   # specify person mean latent dim\n",
    "z_log_sigma = Dense(latent_dim,activation='linear')(e1)              # specify person sd latent dim\n",
    "\n",
    "\n",
    "\n",
    "#sample from latent dim\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "#encoder: connect the input to the latent dim parameters\n",
    "encoder = Model(x, [z_mean, z_log_sigma, z])\n",
    "       \n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,))  # input for the decoder model: the latent dimensions\n",
    "#outputs1 = Dense(latent_dim, activation='linear', kernel_constraint=c_reg,use_bias=False)(latent_inputs)  # the \"cholesky layer\"\n",
    "outputs2 = Dense(nit, activation='sigmoid', kernel_constraint=a_reg2)(latent_inputs)  # connect input to output\n",
    " \n",
    " \n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs2)  # specify the decode model\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(x)[2])    # full output encoder -> decoder -> output\n",
    "vae = Model(x, outputs)\n",
    "\n",
    "\n",
    "# specify the loss function to estimate the model\n",
    "reconstruction_loss = binary_crossentropy(x,outputs)   \n",
    "\n",
    "reconstruction_loss *= nit\n",
    "kl_loss = 1 + 2*z_log_sigma - K.square(z_mean) - K.exp(2*z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    " \n",
    "# fit the model\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-8,patience=500,verbose=1,mode=\"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82737212-df0c-43aa-8947-32f43b7a7ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
